<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>12.3. Primary visibility consensus</title><link rel="stylesheet" type="text/css" href="stylesheet.css" /><link rev="made" href="pgsql-docs@lists.postgresql.org" /><meta name="generator" content="DocBook XSL Stylesheets V1.79.1" /><meta name="keywords" content="ltcluster, LightDB, replication, asynchronous, HA, high-availability" /><link rel="prev" href="ltclusterd-network-split.html" title="12.2. Handling network splits with ltclusterd" /><link rel="next" href="ltclusterd-standby-disconnection-on-failover.html" title="12.4. Standby disconnection on failover" /></head><body><div xmlns="http://www.w3.org/TR/xhtml1/transitional" class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="5" align="center">12.3. Primary visibility consensus</th></tr><tr><td width="10%" align="left"><a accesskey="p" href="ltclusterd-network-split.html" title="12.2. Handling network splits with ltclusterd">Prev</a> </td><td width="10%" align="left"><a accesskey="u" href="ltclusterd-automatic-failover.html" title="Chapter 12. Automatic failover with ltclusterd">Up</a></td><th width="60%" align="center">Chapter 12. Automatic failover with ltclusterd</th><td width="10%" align="right"><a accesskey="h" href="index.html" title="ltcluster 5.2.1 Documentation">Home</a></td><td width="10%" align="right"> <a accesskey="n" href="ltclusterd-standby-disconnection-on-failover.html" title="12.4. Standby disconnection on failover">Next</a></td></tr></table><hr></hr></div><div class="sect1" id="LTCLUSTERD-PRIMARY-VISIBILITY-CONSENSUS"><div class="titlepage"><div><div><h2 class="title" style="clear: both">12.3. Primary visibility consensus</h2></div></div></div><a id="id-1.5.3.6.2" class="indexterm"></a><a id="id-1.5.3.6.3" class="indexterm"></a><p>
    In more complex replication setups, particularly where replication occurs between
    multiple datacentres, it's possible that some but not all standbys get cut off from the
    primary (but not from the other standbys).
  </p><p>
    In this situation, normally it's not desirable for any of the standbys which have been
    cut off to initiate a failover, as the primary is still functioning and standbys are
    connected. Beginning with <a class="link" href="release-4.4.html" title="A.5. Release 4.4"><span class="productname">ltcluster</span> 4.4</a>
    it is now possible for the affected standbys to build a consensus about whether
    the primary is still available to some standbys ("primary visibility consensus").
    This is done by polling each standby (and the witness, if present) for the time it last saw the
    primary; if any have seen the primary very recently, it's reasonable
    to infer that the primary is still available and a failover should not be started.
  </p><p>
    The time the primary was last seen by each node can be checked by executing
    <a class="link" href="ltcluster-service-status.html" title="ltcluster service status"><code class="command">ltcluster service status</code></a>
    (<span class="productname">ltcluster</span> 4.2 - 4.4: <a class="link" href="ltcluster-service-status.html" title="ltcluster service status"><code class="command">ltcluster daemon status</code></a>)
    which includes this in its output, e.g.:
    </p><pre class="programlisting">$ ltcluster -f /etc/ltcluster.conf service status
 ID | Name  | Role    | Status    | Upstream | ltclusterd | PID   | Paused? | Upstream last seen
----+-------+---------+-----------+----------+---------+-------+---------+--------------------
 1  | node1 | primary | * running |          | running | 27259 | no      | n/a
 2  | node2 | standby |   running | node1    | running | 27272 | no      | 1 second(s) ago
 3  | node3 | standby |   running | node1    | running | 27282 | no      | 0 second(s) ago
 4  | node4 | witness | * running | node1    | running | 27298 | no      | 1 second(s) ago</pre><p>

  </p><p>
    To enable this functionality, in <code class="filename">ltcluster.conf</code> set:
    </p><pre class="programlisting">
      primary_visibility_consensus=true</pre><p>
  </p><div class="note"><h3 class="title">Note</h3><p>
      <code class="option">primary_visibility_consensus</code> <span class="emphasis"><em>must</em></span> be set to
      <code class="literal">true</code> on all nodes for it to be effective.
    </p></div><p>
    The following sample <span class="productname">ltclusterd</span> log output demonstrates the behaviour in a situation
    where one of three standbys is no longer able to connect to the primary, but <span class="emphasis"><em>can</em></span>
    connect to the two other standbys ("sibling nodes"):
    </p><pre class="programlisting">
    [2019-05-17 05:36:12] [WARNING] unable to reconnect to node 1 after 3 attempts
    [2019-05-17 05:36:12] [INFO] 2 active sibling nodes registered
    [2019-05-17 05:36:12] [INFO] local node's last receive lsn: 0/7006E58
    [2019-05-17 05:36:12] [INFO] checking state of sibling node "node3" (ID: 3)
    [2019-05-17 05:36:12] [INFO] node "node3" (ID: 3) reports its upstream is node 1, last seen 1 second(s) ago
    [2019-05-17 05:36:12] [NOTICE] node 3 last saw primary node 1 second(s) ago, considering primary still visible
    [2019-05-17 05:36:12] [INFO] last receive LSN for sibling node "node3" (ID: 3) is: 0/7006E58
    [2019-05-17 05:36:12] [INFO] node "node3" (ID: 3) has same LSN as current candidate "node2" (ID: 2)
    [2019-05-17 05:36:12] [INFO] checking state of sibling node "node4" (ID: 4)
    [2019-05-17 05:36:12] [INFO] node "node4" (ID: 4) reports its upstream is node 1, last seen 0 second(s) ago
    [2019-05-17 05:36:12] [NOTICE] node 4 last saw primary node 0 second(s) ago, considering primary still visible
    [2019-05-17 05:36:12] [INFO] last receive LSN for sibling node "node4" (ID: 4) is: 0/7006E58
    [2019-05-17 05:36:12] [INFO] node "node4" (ID: 4) has same LSN as current candidate "node2" (ID: 2)
    [2019-05-17 05:36:12] [INFO] 2 nodes can see the primary
    [2019-05-17 05:36:12] [DETAIL] following nodes can see the primary:
     - node "node3" (ID: 3): 1 second(s) ago
     - node "node4" (ID: 4): 0 second(s) ago
    [2019-05-17 05:36:12] [NOTICE] cancelling failover as some nodes can still see the primary
    [2019-05-17 05:36:12] [NOTICE] election cancelled
    [2019-05-17 05:36:14] [INFO] node "node2" (ID: 2) monitoring upstream node "node1" (ID: 1) in degraded state</pre><p>
    In this situation it will cancel the failover and enter degraded monitoring node,
    waiting for the primary to reappear.
  </p></div><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="ltclusterd-network-split.html">Prev</a> </td><td width="20%" align="center"><a accesskey="u" href="ltclusterd-automatic-failover.html">Up</a></td><td width="40%" align="right"> <a accesskey="n" href="ltclusterd-standby-disconnection-on-failover.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">12.2. Handling network splits with ltclusterd </td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top"> 12.4. Standby disconnection on failover</td></tr></table></div></body></html>